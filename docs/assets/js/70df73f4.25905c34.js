"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[341],{3905:(e,r,a)=>{a.d(r,{Zo:()=>s,kt:()=>f});var n=a(7294);function t(e,r,a){return r in e?Object.defineProperty(e,r,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[r]=a,e}function l(e,r){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var r=1;r<arguments.length;r++){var a=null!=arguments[r]?arguments[r]:{};r%2?l(Object(a),!0).forEach((function(r){t(e,r,a[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(a,r))}))}return e}function i(e,r){if(null==e)return{};var a,n,t=function(e,r){if(null==e)return{};var a,n,t={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],r.indexOf(a)>=0||(t[a]=e[a]);return t}(e,r);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],r.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var c=n.createContext({}),p=function(e){var r=n.useContext(c),a=r;return e&&(a="function"==typeof e?e(r):o(o({},r),e)),a},s=function(e){var r=p(e.components);return n.createElement(c.Provider,{value:r},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},d=n.forwardRef((function(e,r){var a=e.components,t=e.mdxType,l=e.originalType,c=e.parentName,s=i(e,["components","mdxType","originalType","parentName"]),u=p(a),d=t,f=u["".concat(c,".").concat(d)]||u[d]||m[d]||l;return a?n.createElement(f,o(o({ref:r},s),{},{components:a})):n.createElement(f,o({ref:r},s))}));function f(e,r){var a=arguments,t=r&&r.mdxType;if("string"==typeof e||t){var l=a.length,o=new Array(l);o[0]=d;var i={};for(var c in r)hasOwnProperty.call(r,c)&&(i[c]=r[c]);i.originalType=e,i[u]="string"==typeof e?e:t,o[1]=i;for(var p=2;p<l;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},1138:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>l,metadata:()=>i,toc:()=>p});var n=a(7462),t=(a(7294),a(3905));const l={sidebar_position:5,tags:["Linux","llama.cpp","offline","ai"]},o="llama.cpp",i={unversionedId:"ai/llamacpp",id:"ai/llamacpp",title:"llama.cpp",description:"Ollama is an open-source AI model server. It can get and run large language models (LLMs) locally on your machine.",source:"@site/docs/ai/llamacpp.md",sourceDirName:"ai",slug:"/ai/llamacpp",permalink:"/Wisdom-Hub/ai/llamacpp",draft:!1,tags:[{label:"Linux",permalink:"/Wisdom-Hub/tags/linux"},{label:"llama.cpp",permalink:"/Wisdom-Hub/tags/llama-cpp"},{label:"offline",permalink:"/Wisdom-Hub/tags/offline"},{label:"ai",permalink:"/Wisdom-Hub/tags/ai"}],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,tags:["Linux","llama.cpp","offline","ai"]},sidebar:"tutorialSidebar",previous:{title:"AI",permalink:"/Wisdom-Hub/ai/"},next:{title:"Ollama",permalink:"/Wisdom-Hub/ai/ollama"}},c={},p=[{value:"Rocm",id:"rocm",level:2},{value:"Arch",id:"arch",level:3},{value:"Old GPUs",id:"old-gpus",level:3},{value:"Install",id:"install",level:3}],s={toc:p},u="wrapper";function m(e){let{components:r,...a}=e;return(0,t.kt)(u,(0,n.Z)({},s,a,{components:r,mdxType:"MDXLayout"}),(0,t.kt)("h1",{id:"llamacpp"},"llama.cpp"),(0,t.kt)("p",null,"Ollama is an open-source AI model server. It can get and run large language models (LLMs) locally on your machine."),(0,t.kt)("h2",{id:"rocm"},"Rocm"),(0,t.kt)("p",null,"First make sure you have rocm or NVIDIA CUDA installed."),(0,t.kt)("h3",{id:"arch"},"Arch"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-bash"},"sudo pacman -Syuu base-devel cmake gcc python3 rocm-hip-sdk  rocm-opencl-sdk rocm-opencl-runtime rocm-ml-libraries rocm-device-libs\n\nsudo usermod -a -G render,video $(whoami)\nsudo reboot\nrocminfo\n")),(0,t.kt)("h3",{id:"old-gpus"},"Old GPUs"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-bash"},"# If you are using RDNA or RDNA 2 architecture like AMD Radeon RX 6500 XT, you may need to follow this step:\nsudo nano ~/.profile\n# Add the following lines:\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROC_ENABLE_PRE_VEGA=1\nexport ROCM_PATH=/opt/rocm\n")),(0,t.kt)("h3",{id:"install"},"Install"),(0,t.kt)("pre",null,(0,t.kt)("code",{parentName:"pre",className:"language-bash"},"pamac install llama.cpp-hip\n")))}m.isMDXComponent=!0}}]);